# Proyecto 4: Tutor Inteligente para Algoritmos con Fine-Tuning LoRA

## üìå Descripci√≥n
Este proyecto implementa un sistema de tutor√≠a inteligente especializado en algoritmos y estructuras de datos, utilizando fine-tuning con LoRA (Low-Rank Adaptation) sobre el modelo Llama 3.2. El sistema es capaz de responder preguntas, generar pseudoc√≥digo, explicar conceptos complejos y corregir errores algor√≠tmicos, todo basado en un dataset especializado de preguntas y respuestas.

## üéØ Objetivos
- Implementar fine-tuning con LoRA para adaptar un modelo de lenguaje a un dominio espec√≠fico (algoritmos)
- Crear un dataset especializado de tutor√≠a en algoritmos y estructuras de datos
- Desarrollar un pipeline completo desde el entrenamiento hasta el despliegue con Ollama
- Optimizar el modelo para ejecuci√≥n local eficiente mediante cuantizaci√≥n GGUF
- Construir un sistema de tutor√≠a interactivo que responda con precisi√≥n t√©cnica

## üõ†Ô∏è Tecnolog√≠as Utilizadas
- **Modelo base:** Llama 3.2 3B Instruct (Meta)
- **Framework:** Hugging Face Transformers, PEFT (LoRA)
- **Entrenamiento:** bitsandbytes (8-bit quantization), PyTorch
- **Dataset:** JSONL personalizado con 200+ pares instrucci√≥n-respuesta
- **Conversi√≥n:** llama.cpp para formato GGUF cuantizado
- **Despliegue:** Ollama con archivo Modelfile personalizado

## üöÄ Instalaci√≥n y Ejecuci√≥n

### Prerrequisitos
```bash
# Instalaci√≥n de dependencias principales
pip install torch transformers datasets peft bitsandbytes accelerate
pip install sentencepiece protobuf

# Para conversi√≥n GGUF
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp && make

# Ollama (descargar desde ollama.ai)
```

### Estructura del Proyecto
```
tutor_dataset.jsonl           # Dataset de entrenamiento (200+ ejemplos)
train_lora.py                 # Script de entrenamiento con LoRA
convert.py                    # Conversi√≥n a formato GGUF
Modelfile                     # Configuraci√≥n para Ollama (sin extensi√≥n)
lora-tutor/                   # Adaptadores LoRA entrenados
llama_lora.gguf              # Modelo fusionado y cuantizado
```

### Flujo de Ejecuci√≥n Completo:

#### Paso 1: Preparar el Dataset
- **Formato:** JSONL con pares `{"instruction": "...", "response": "..."}`
- **Contenido:** 200+ preguntas y respuestas sobre algoritmos, estructuras de datos, complejidad temporal
- **Ejemplo:**
```json
{"instruction": "Explica la complejidad temporal de la b√∫squeda binaria.", "response": "La b√∫squeda binaria tiene una complejidad temporal de O(log n). Esto se debe a que el espacio de b√∫squeda se reduce a la mitad en cada iteraci√≥n..."}
```

#### Paso 2: Entrenamiento con LoRA
```bash
python train_lora.py
```
**Par√°metros de entrenamiento:**
- **Modelo base:** `meta-llama/Llama-3.2-3B-Instruct`
- **LoRA rank (r):** 8 (bajo para eficiencia)
- **LoRA alpha:** 16 (factor de escalado)
- **M√≥dulos objetivo:** q_proj, k_proj, v_proj, o_proj (attention layers)
- **Batch size efectivo:** 16 (gradient accumulation)
- **√âpocas:** 3
- **Quantization:** 8-bit (bitsandbytes)

#### Paso 3: Conversi√≥n a GGUF
```bash
python convert.py --base meta-llama/Llama-3.2-3B-Instruct --model ./lora-tutor --out llama_lora.gguf --dtype float16
```
**Proceso de conversi√≥n:**
1. Fusiona adaptadores LoRA con modelo base
2. Guarda modelo fusionado temporalmente
3. Convierte a formato GGUF con cuantizaci√≥n Q8_0
4. Genera archivo `llama_lora.gguf` optimizado

#### Paso 4: Crear Modelo Ollama
```bash
ollama create tutor-algoritmos -f Modelfile
```
**Contenido del Modelfile:**
- **FROM:** Modelo base Llama 3.2
- **ADAPTER:** Archivo GGUF fusionado
- **SYSTEM:** Prompt espec√≠fico para tutor√≠a algor√≠tmica
- **PARAMETERS:** temperature=0.2 (baja creatividad), top_p=0.9

#### Paso 5: Ejecutar el Tutor
```bash
ollama run tutor-algoritmos
```
**Ejemplo de interacci√≥n:**
```
Usuario: ¬øCu√°l es la complejidad de Bubble Sort?
Tutor: Bubble Sort tiene complejidad O(n¬≤) en el peor caso. Esto se debe a que...
```

## üìä Metodolog√≠a

### 1. Dataset Especializado
- **200+ ejemplos cuidadosamente elaborados**
- **Temas cubiertos:** Complejidad algor√≠tmica, estructuras de datos, grafos, √°rboles, b√∫squeda, ordenamiento
- **Formato estandarizado:** Instrucci√≥n clara + respuesta t√©cnica completa
- **Incluye:** Pseudoc√≥digo, f√≥rmulas matem√°ticas, explicaciones paso a paso

### 2. Fine-Tuning con LoRA
- **Adaptaci√≥n eficiente:** Solo 0.1% de par√°metros entrenados (~8M de 8B)
- **Configuraci√≥n LoRA:**
  - `r=8`: Dimensi√≥n de baja jerarqu√≠a (balance calidad/eficiencia)
  - `lora_alpha=16`: Factor de escalado para adaptadores
  - `target_modules`: Capas de atenci√≥n (q_proj, k_proj, v_proj, o_proj)
  - `lora_dropout=0.1`: Regularizaci√≥n para evitar overfitting
- **Entrenamiento optimizado:**
  - 8-bit quantization para ahorrar memoria
  - Gradient accumulation (batch size efectivo 16)
  - FP16 mixed precision
  - 3 √©pocas para convergencia adecuada

### 3. Pipeline de Conversi√≥n
1. **Fusi√≥n LoRA:** Merge adaptadores con modelo base
2. **Serializaci√≥n segura:** Guardado en formato safetensors
3. **Conversi√≥n GGUF:** Optimizado para llama.cpp
4. **Cuantizaci√≥n Q8_0:** 8-bit quantization (balance precisi√≥n/velocidad)

### 4. Sistema de Prompt Engineering
- **System prompt especializado:** Define rol de tutor algor√≠tmico
- **Par√°metros de inferencia:**
  - `temperature=0.2`: Respuestas consistentes y t√©cnicas
  - `top_p=0.9`: Balance entre creatividad y precisi√≥n
- **Contexto restringido:** Solo utiliza conocimiento del dataset

### 5. Optimizaciones de Rendimiento
- **Memoria eficiente:** 8-bit quantization durante entrenamiento
- **Inferencia r√°pida:** Formato GGUF optimizado para CPU/GPU
- **Modelo liviano:** ~3.5GB (vs 12GB del modelo original)
- **Respuesta en tiempo real:** Latencia baja para interacci√≥n fluida

## üñºÔ∏è Evidencias

### Interacci√≥n con el tutor
![prueba1](https://github.com/user-attachments/assets/ba2abab9-1df3-43ce-958e-719f35dd3521)

![prueba2](https://github.com/user-attachments/assets/417624c8-e345-4486-96c4-04b825516e47)

![prueba3](https://github.com/user-attachments/assets/e30c4d43-2c61-4c3d-8a52-34091cccda80)

